# Newspaper Explorer

A tool for exploring and analyzing historical newspaper data from the "Der Tag" collection.

## ğŸ“¦ Installation

### Using pip

```bash
# Clone the repository
git clone https://github.com/newspaper-collective/newspaper_explorer.git
cd newspaper_explorer

# Copy environment template
cp .env.example .env

# Install in development mode
pip install -e .
```

### Using uv (recommended - faster!)

```bash
# Install uv if needed
pip install uv

# Clone and setup
git clone https://github.com/newspaper-collective/newspaper_explorer.git
cd newspaper_explorer

# Copy environment template
cp .env.example .env

# Create venv and install
uv venv
.venv\Scripts\Activate.ps1  # Windows PowerShell
# or: source .venv/bin/activate  # Linux/Mac

uv pip install -e .
```

See [INSTALL.md](INSTALL.md) for detailed installation instructions.

## ğŸš€ Quick Start

### Using the CLI

```bash
# List available dataset parts
newspaper-explorer data list

# Check current status
newspaper-explorer data status

# Download and extract a specific time period
newspaper-explorer data download --part dertag_1900-1902

# Download multiple parts
newspaper-explorer data download --parts dertag_1900-1902,dertag_1903-1905

# Download all parts at once
newspaper-explorer data download --all

# Download without extraction
newspaper-explorer data download --part dertag_1900-1902 --no-extract

# Download without automatic error fixes
newspaper-explorer data download --part dertag_1900-1902 --no-fix

# Parallel downloads (faster for multiple parts)
newspaper-explorer data download --parts dertag_1900-1902,dertag_1903-1905 --parallel

# Extract already downloaded data
newspaper-explorer data extract --part dertag_1900-1902

# Verify checksums
newspaper-explorer data verify --part dertag_1900-1902

# Get help
newspaper-explorer --help
newspaper-explorer data download --help
```

### Using the Python API

```python
from newspaper_explorer.data.download import ZenodoDownloader

# Initialize downloader
downloader = ZenodoDownloader()

# List available parts
for part in downloader.list_available_parts():
    print(f"{part['name']} - {part['years']}")

# Download and extract a single part
downloader.download_and_extract(['dertag_1900-1902'])

# Download multiple parts in parallel
downloader.download_and_extract(
    ['dertag_1900-1902', 'dertag_1903-1905'],
    parallel=True
)

# Download without extraction
downloader.download_part('dertag_1900-1902')

# Extract already downloaded data
downloader.extract_part('dertag_1900-1902')

# Check status
downloader.print_status_summary()
```

## ğŸ“š Documentation

- [CLI Reference](docs/CLI.md) - Complete CLI command reference
- [Data Management Guide](docs/DATA.md) - Detailed guide for data downloading, extraction, and source configuration
- [Installation Guide](docs/INSTALL.md) - Setup instructions for pip and uv
- [Configuration Philosophy](docs/CONFIGURATION_PHILOSOPHY.md) - Design decisions for config vs CLI flags

## ğŸ—‚ï¸ Project Structure

```
newspaper_explorer/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ newspaper_explorer/
â”‚       â”œâ”€â”€ main.py              # Main CLI entry point
â”‚       â”œâ”€â”€ cli/                 # CLI command modules (no __init__.py)
â”‚       â”‚   â””â”€â”€ data.py          # Data management commands
â”‚       â”œâ”€â”€ data/                # Data handling (no __init__.py)
â”‚       â”‚   â”œâ”€â”€ download.py      # Download/extract functionality
â”‚       â”‚   â”œâ”€â”€ fixes.py         # Error correction utilities
â”‚       â”‚   â”œâ”€â”€ loading.py       # Data loading (placeholder)
â”‚       â”‚   â””â”€â”€ cleaning.py      # Data cleaning (placeholder)
â”‚       â”œâ”€â”€ analysis/            # Analysis modules (placeholders)
â”‚       â”‚   â”œâ”€â”€ concepts/
â”‚       â”‚   â”œâ”€â”€ emotions/
â”‚       â”‚   â”œâ”€â”€ entities/
â”‚       â”‚   â”œâ”€â”€ layout/
â”‚       â”‚   â””â”€â”€ topics/
â”‚       â”œâ”€â”€ ui/                  # UI components (future)
â”‚       â””â”€â”€ utils/               # Utilities (no __init__.py)
â”‚           â””â”€â”€ config.py        # Configuration management
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sources/
â”‚   â”‚   â””â”€â”€ der_tag.json         # Dataset configuration with checksums
â”‚   â”œâ”€â”€ downloads/               # Downloaded .tar.gz files (preserved)
â”‚   â”œâ”€â”€ extracted/               # Temp extraction (auto-cleaned, kept empty)
â”‚   â””â”€â”€ raw/                     # Organized data by year
â”œâ”€â”€ results/                     # Analysis results and outputs
â”œâ”€â”€ docs/                        # Documentation
â””â”€â”€ tests/                       # Test suite
```

**Important Notes:**

- This project uses **explicit imports** without `__init__.py` files
- Use full module paths: `from newspaper_explorer.utils.config import get_config`
- See `.github/copilot-instructions.md` for coding guidelines
- Analysis modules (`analysis/`, `data/cleaning.py`, `data/loading.py`) are placeholders for future development

## ğŸ¯ Features

### Data Management

- âœ… Download newspaper data from Zenodo
- âœ… MD5 checksum verification
- âœ… Automatic extraction of tar.gz archives
- âœ… Smart caching (won't re-download existing files)
- âœ… Progress bars for downloads
- âœ… Automatic error correction for known data issues
- âœ… Status tracking and reporting
- âœ… Parallel downloads for faster multi-part downloads
- âœ… Configurable directories via environment variables

### CLI Commands

- `data list` - List all available dataset parts with metadata
- `data status` - Show download/extraction status (detailed with `--verbose`)
- `data download` - Download one or more parts (with `--all`, `--parallel`, `--no-extract`, `--no-fix` options)
- `data extract` - Extract already downloaded archives
- `data verify` - Verify MD5 checksums of downloaded files

### Analysis Modules (Planned)

- ğŸ“ Text cleaning and normalization
- ğŸ“ Data loading from XML/OCR files
- ğŸ“ Concept extraction
- ğŸ“ Emotion analysis
- ğŸ“ Entity recognition
- ğŸ“ Layout analysis
- ğŸ“ Topic modeling

## ğŸ“Š Available Dataset

The "Der Tag" newspaper collection from Zenodo.

**Collection**: [Der Tag on Zenodo](https://zenodo.org/records/17232177)

Use `newspaper-explorer data list` to see all available dataset parts with years, sizes, and checksums.

## ğŸ› ï¸ Development

```bash
# Install development dependencies
pip install -e ".[dev]"

# Run tests (when implemented)
pytest

# Format code
black src/

# Lint code
ruff check src/
```

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For questions or issues, please open an issue on GitHub.
